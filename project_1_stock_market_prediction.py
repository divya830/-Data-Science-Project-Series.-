# -*- coding: utf-8 -*-
"""Project 1: Stock Market Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f8Y8d02Giwtc2tN9Xm0MymceDTgICf3o
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve

from sklearn.neighbors import KNeighborsRegressor

from sklearn.neural_network import MLPClassifier
import pandas as pd
import os
import time
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import GroupKFold
import lightgbm as lgb
from sklearn import metrics

#Importing
df = pd.read_csv("/content/drive/MyDrive/infolimpioavanzadoTarget.csv")

df.head()

df.info()

"""NA Values"""

import pandas as pd

# Assuming df is your DataFrame

# Set display options to show all rows and columns
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

# Print the count of NaN values for each column
print(df.isna().sum())

# Reset display options to their default values if needed
pd.reset_option('display.max_rows')
pd.reset_option('display.max_columns')

for column in df.columns:
    print(column, df[column].isna().sum()/df.shape[0]) #returns the fraction of NAN values

"""Summary stats"""

df.describe()

"""histbins"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Assuming df is your DataFrame

# Identify columns without inf or -inf values
finite_columns = ~df.isin([np.inf, -np.inf]).any()

# Filter the DataFrame based on the identified columns
df_no_inf = df.loc[:, finite_columns]

# Retry the histogram
df_no_inf.hist(bins=50, figsize=(100, 50))

# Show the plot
plt.show()

"""outliers"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Assuming your DataFrame is named df
# Replace df with the actual name of your DataFrame

# Create boxplots for each numeric column
for column in df.select_dtypes(include=np.number).columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=df[column])
    plt.title(f'Boxplot for {column}')
    plt.show()

# Function to detect outliers using IQR method
def find_outliers_iqr(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = (series < lower_bound) | (series > upper_bound)
    return outliers

# Iterate through each numeric column
for column in df.select_dtypes(include=np.number).columns:
    # Find outliers using the IQR method
    outliers = find_outliers_iqr(df[column])

    # Print the number of outliers for each column
    print(f"Number of outliers in {column}: {outliers.sum()}")

"""Distribution"""

# Create histograms for each numeric column
for column in df.select_dtypes(include=np.number).columns:
    plt.figure(figsize=(8, 6))
    sns.histplot(df[column], kde=True)
    plt.title(f'Histogram for {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.show()

"""Unique Values"""

# Get the number of unique values for each column
unique_counts = df.nunique()

# Print the results
for column, count in unique_counts.items():
    print(f"Column '{column}' has {count} unique values.")

"""Correlation"""

## correlation
corr_matrix = df.corr()
corr_matrix.columns
corr_matrix['TARGET'].sort_values(ascending = False)

df['TARGET'].info()

# Assuming df is your main DataFrame
pathOutput = './'

# Separate actual results for comparison
columns = [col for col in df.columns if col not in ['company', 'age', 'market', 'TARGET']]
submission = df[columns]
solucion = df['TARGET']

# Check if 'close' is present in the DataFrame
if 'close' in df.columns:
    # Calculate the 20-day return for each row
    df['close_20dias'] = df['close'].shift(-20)
    # Continue with your calculations or analysis using the 'close_20dias' column
else:
    print("The 'close' column is not present in the DataFrame.")

# Define the calculate_percentage_return function
def calculate_percentage_return(row):
    if row['close_20dias'] is not None and row['close'] is not None:
        return (100 * (row['close_20dias'] - row['close'])) / row['close']
    else:
        return None

# Generate the 20-day return column
df['renta_20dias'] = df.apply(calculate_percentage_return, axis=1)

# Filter out rows with null values in the 'TARGET' column
def filter_rows_by_values(df, col, values):
    return df[~df[col].isin(values)]

df = filter_rows_by_values(df, "TARGET", ["null"])

# NEW FEATURES
# RSI Calculation
def relative_strength_idx(df, n=14):
    close = df['close']
    delta = close.diff()
    delta = delta[1:]
    pricesUp = delta.copy()
    pricesDown = delta.copy()
    pricesUp[pricesUp < 0] = 0
    pricesDown[pricesDown > 0] = 0
    rollUp = pricesUp.rolling(n).mean()
    rollDown = pricesDown.abs().rolling(n).mean()
    rs = rollUp / rollDown
    rsi = 100.0 - (100.0 / (1.0 + rs))
    return rsi

# NEW FEATURES FOR TRAIN
df['close_lag'] = df['close'].shift(1)
df['RSI'] = relative_strength_idx(df).fillna(0)
df = df.fillna(0)

# Split the data into train and validation
train_fraction = 0.7
train = df.sample(frac=train_fraction)
validation = df.drop(train.index)

# Separate features and target
train_X = train[columns]
train_y = train['TARGET']
valid_X = validation[columns]
valid_y = validation['TARGET']

# Drop non-numeric columns
numeric_columns = train_X.select_dtypes(include=['int', 'float', 'bool']).columns
train_X = train_X[numeric_columns]
valid_X = valid_X[numeric_columns]

def plot(predictions, actual_values):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 7))

    # Distribution plot
    sns.distplot(actual_values, label='actual values', ax=ax1)
    sns.distplot(predictions, label='predictions', ax=ax1)
    ax1.set_xlabel('Distribution plot')

    # Scatter plot
    ax2.scatter(actual_values, predictions, c='orange', label='predictions')
    ax2.plot(actual_values, actual_values, c='blue', label='y=x')
    ax2.set_xlabel('Actual values')
    ax2.set_ylabel('Predicted values')

    ax1.legend()
    ax2.legend()
    ax2.axis('scaled')  # Same x y scale

    plt.show()

"""LGBM"""

# MODEL TRAINING
# LGBM MODEL
from sklearn.model_selection import KFold

# Assuming train_X, train_y, valid_X, valid_y are defined

folds = KFold(n_splits=5, shuffle=True, random_state=42)  # Use regular KFold instead of GroupKFold

params = {
    'objective': 'binary',
    'learning_rate': 0.02,
    'boosting_type': 'gbdt',
    'metric': 'precision',
    'n_jobs': -1,
    'min_data_in_leaf': 32,
    'num_leaves': 1024,
}

for fold_n, (train_index, valid_index) in enumerate(folds.split(train_X, train_y)):
    print(f'Fold {fold_n} started at {time.ctime()}')
    X_train, X_valid = train_X.iloc[train_index], train_X.iloc[valid_index]
    y_train, y_valid = train_y.iloc[train_index], train_y.iloc[valid_index]

    model = lgb.LGBMClassifier(**params, n_estimators=50)
    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)])

# FEATURE IMPORTANCE PLOTTING
feature_importance = pd.DataFrame()
fold_importance = pd.DataFrame()
fold_importance["feature"] = train_X.columns  # Use train_X columns directly
fold_importance["importance"] = model.feature_importances_
fold_importance["fold"] = fold_n + 1
feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

feature_importance["importance"] /= 5
# Plot the top 50 features
cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
    by="importance", ascending=False)[:50].index
best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

plt.figure(figsize=(16, 12))
sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False))
plt.title('LGB Features (avg over folds)')
plt.savefig(pathOutput + "BOLSA_feature_importances.png")
###################

##################### VALIDATION ###################
print("START OF VALIDATION")
score = metrics.mean_absolute_error(valid_y, model.predict(valid_X))
print('CV score: {0:.4f}.'.format(score))
print("END OF VALIDATION")

plot(model.predict(valid_X),valid_y)

"""Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.impute import SimpleImputer
from sklearn.model_selection import KFold

# Assuming train_X, train_y, valid_X, valid_y are defined

folds = KFold(n_splits=5, shuffle=True, random_state=42)  # Use regular KFold instead of GroupKFold

params = {
    'criterion': 'gini',  # or 'entropy'
    'max_depth': None,  # Set the maximum depth of the tree
    'min_samples_split': 2,
    'min_samples_leaf': 1,
    'max_features': None,  # Set the maximum number of features to consider for splitting
    'random_state': 42,
}

# Replace infinity with NaN and then impute NaN values
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)

# Impute NaN values using a strategy (mean, median, etc.)
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_valid_imputed = imputer.transform(X_valid)

# MODEL TRAINING
# Decision Tree MODEL

from sklearn.model_selection import train_test_split

for fold_n, (train_index, valid_index) in enumerate(folds.split(train_X, train_y)):
    print(f'Fold {fold_n} started at {time.ctime()}')

    # Split the data using train_test_split
    X_train_imputed_fold, X_valid_imputed_fold, y_train_fold, y_valid_fold = train_test_split(
        X_train_imputed, y_train, test_size=0.2, random_state=42
    )

    model = DecisionTreeClassifier(**params)
    model.fit(X_train_imputed_fold, y_train_fold)

# FEATURE IMPORTANCE PLOTTING
feature_importance = pd.DataFrame()
fold_importance = pd.DataFrame()
fold_importance["feature"] = train_X.columns  # Use train_X columns directly
fold_importance["importance"] = model.feature_importances_
fold_importance["fold"] = fold_n + 1
feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

# Plot the top 50 features
cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
    by="importance", ascending=False)[:50].index
best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

plt.figure(figsize=(16, 12))
sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False))
plt.title('Decision Tree Features (avg over folds)')
plt.savefig(pathOutput + "BOLSA_feature_importances_decision_tree.png")

# Predict on the validation set
predictions = model.predict(X_valid_imputed_fold)

# Calculate accuracy
accuracy = (predictions == y_valid_fold).mean()
print(f'Accuracy: {accuracy:.4f}')

plot(predictions, y_valid_fold)

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import SimpleImputer
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import accuracy_score

for fold_n, (train_index, valid_index) in enumerate(folds.split(train_X, train_y)):
    print(f'Fold {fold_n} started at {time.ctime()}')

    # Split the data using train_test_split
    X_train_imputed_fold, X_valid_imputed_fold, y_train_fold, y_valid_fold = train_test_split(
        X_train_imputed, y_train, test_size=0.2, random_state=42
    )

    # Create and train the KNN model
    model = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors as needed
    model.fit(X_train_imputed_fold, y_train_fold)

    # Predict on the validation set
    predictions = model.predict(X_valid_imputed_fold)

    # Calculate accuracy
    accuracy = accuracy_score(y_valid_fold, predictions)
    print(f'Accuracy: {accuracy:.4f}')

plot(predictions, y_valid_fold)

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import accuracy_score


for fold_n, (train_index, valid_index) in enumerate(folds.split(train_X, train_y)):
    print(f'Fold {fold_n} started at {time.ctime()}')

    # Split the data using train_test_split
    X_train_imputed_fold, X_valid_imputed_fold, y_train_fold, y_valid_fold = train_test_split(
        X_train_imputed, y_train, test_size=0.2, random_state=42
    )

    # Create and train the Random Forest model
    model = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust the number of estimators as needed
    model.fit(X_train_imputed_fold, y_train_fold)

    # Predict on the validation set
    predictions = model.predict(X_valid_imputed_fold)

    # Calculate accuracy
    accuracy = accuracy_score(y_valid_fold, predictions)
    print(f'Accuracy: {accuracy:.4f}')

accuracy = accuracy_score(y_valid_fold, predictions)
print(f'Accuracy: {accuracy:.4f}')

plot(predictions, y_valid_fold)

"""XgBoost"""

import xgboost as xgb
from sklearn.impute import SimpleImputer
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import accuracy_score


for fold_n, (train_index, valid_index) in enumerate(folds.split(train_X, train_y)):
    print(f'Fold {fold_n} started at {time.ctime()}')

    # Split the data using train_test_split
    X_train_imputed_fold, X_valid_imputed_fold, y_train_fold, y_valid_fold = train_test_split(
        X_train_imputed, y_train, test_size=0.2, random_state=42
    )

    # Create and train the XGBoost model
    model = xgb.XGBClassifier(n_estimators=100, random_state=42)  # You can adjust the number of estimators as needed
    model.fit(X_train_imputed_fold, y_train_fold)

    # Predict on the validation set
    predictions = model.predict(X_valid_imputed_fold)

    # Calculate accuracy
    accuracy = accuracy_score(y_valid_fold, predictions)
    print(f'Accuracy: {accuracy:.4f}')

plot(predictions, y_valid_fold)

"""Adaboost"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.impute import SimpleImputer
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import accuracy_score


for fold_n, (train_index, valid_index) in enumerate(folds.split(train_X, train_y)):
    print(f'Fold {fold_n} started at {time.ctime()}')

    # Split the data using train_test_split
    X_train_imputed_fold, X_valid_imputed_fold, y_train_fold, y_valid_fold = train_test_split(
        X_train_imputed, y_train, test_size=0.2, random_state=42
    )

    # Create and train the AdaBoost model
    model = AdaBoostClassifier(n_estimators=50, random_state=42)  # You can adjust the number of estimators as needed
    model.fit(X_train_imputed_fold, y_train_fold)

    # Predict on the validation set
    predictions = model.predict(X_valid_imputed_fold)

    # Calculate accuracy
    accuracy = accuracy_score(y_valid_fold, predictions)
    print(f'Accuracy: {accuracy:.4f}')

plot(predictions, y_valid_fold)

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import accuracy_score


for fold_n, (train_index, valid_index) in enumerate(folds.split(train_X, train_y)):
    print(f'Fold {fold_n} started at {time.ctime()}')

    # Split the data using train_test_split
    X_train_imputed_fold, X_valid_imputed_fold, y_train_fold, y_valid_fold = train_test_split(
        X_train_imputed, y_train, test_size=0.2, random_state=42
    )

    # Create and train the Logistic Regression model
    model = LogisticRegression(random_state=42)
    model.fit(X_train_imputed_fold, y_train_fold)

    # Predict on the validation set
    predictions = model.predict(X_valid_imputed_fold)

    # Calculate accuracy
    accuracy = accuracy_score(y_valid_fold, predictions)
    print(f'Accuracy: {accuracy:.4f}')

plot(predictions, y_valid_fold)

"""Neural"""

from sklearn.neural_network import MLPClassifier
from sklearn.impute import SimpleImputer
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import accuracy_score


for fold_n, (train_index, valid_index) in enumerate(folds.split(train_X, train_y)):
    print(f'Fold {fold_n} started at {time.ctime()}')

    # Split the data using train_test_split
    X_train_imputed_fold, X_valid_imputed_fold, y_train_fold, y_valid_fold = train_test_split(
        X_train_imputed, y_train, test_size=0.2, random_state=42
    )

    # Create and train the Neural Network model
    model = MLPClassifier(hidden_layer_sizes=(100, ), max_iter=100, random_state=42)
    model.fit(X_train_imputed_fold, y_train_fold)

    # Predict on the validation set
    predictions = model.predict(X_valid_imputed_fold)

    # Calculate accuracy
    accuracy = accuracy_score(y_valid_fold, predictions)
    print(f'Accuracy: {accuracy:.4f}')

plot(predictions, y_valid_fold)